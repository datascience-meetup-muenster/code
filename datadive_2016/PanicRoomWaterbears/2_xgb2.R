
#----------------------------------------------
# Our best model on the private leaderboard
#-----------------------------------------------

library(tidyverse)
library(caret)
library(doSNOW)

cl <- makeCluster(parallel::detectCores(), outfile = "")
registerDoSNOW(cl)

load("data/trainset2.RData")
load("data/testset2.RData")

#
# Fit and validate
#
y_train <- trainset2$visitors_pool_total
x_train <- dplyr::select(trainset2, -sunshine_hours_DWD, -air_pressure_UniOS, -wday,
                         -visitors_pool_total, -date, -weeknr_sq, -monthnr_sq)
x_test <- dplyr::select(testset2, -sunshine_hours_DWD, -air_pressure_UniOS, -wday,
                        -date, -weeknr_sq, -monthnr_sq)

my_trcontrol <- trainControl(method = "repeatedcv", allowParallel = T, number = 7,
                             repeats = 1, savePredictions = T, verboseIter = T)
mod <- train(y = y_train, x = x_train, method = "xgbTree",
             tuneLength = 5,
             preProcess = "knnImpute", trControl = my_trcontrol)
min(mod$results$RMSE) # 272.8
mod
# Xtreme Gradient Boosting
#
# 2835 samples
# 97 predictor
#
# Pre-processing: nearest neighbor imputation (97), centered (97), scaled (97)
# Resampling: Cross-Validated (7 fold, repeated 1 times)
# Summary of sample sizes: 2431, 2430, 2430, 2430, 2429, 2430, ...
# Resampling results across tuning parameters:
#
#     eta  max_depth  colsample_bytree  nrounds  RMSE      Rsquared
# 0.3  1          0.6                50      357.9356  0.6530605
# 0.3  1          0.6               100      343.6569  0.6765386
# 0.3  1          0.6               150      337.3923  0.6868381
# 0.3  1          0.6               200      334.4031  0.6921077
# 0.3  1          0.6               250      332.4495  0.6957089
# 0.3  1          0.8                50      358.1809  0.6534061
# 0.3  1          0.8               100      343.2037  0.6777597
# 0.3  1          0.8               150      337.3664  0.6868896
# 0.3  1          0.8               200      334.3239  0.6922179
# 0.3  1          0.8               250      332.5749  0.6953766
# 0.3  2          0.6                50      320.4332  0.7191333
# 0.3  2          0.6               100      305.2303  0.7445144
# 0.3  2          0.6               150      297.5234  0.7570519
# 0.3  2          0.6               200      294.4473  0.7618514
# 0.3  2          0.6               250      292.3317  0.7650765
# 0.3  2          0.8                50      322.0974  0.7160431
# 0.3  2          0.8               100      303.0722  0.7486959
# 0.3  2          0.8               150      295.5476  0.7606397
# 0.3  2          0.8               200      291.8224  0.7664281
# 0.3  2          0.8               250      289.3599  0.7703749
# 0.3  3          0.6                50      299.1586  0.7543638
# 0.3  3          0.6               100      285.8813  0.7752675
# 0.3  3          0.6               150      283.0313  0.7795388
# 0.3  3          0.6               200      281.5359  0.7817759
# 0.3  3          0.6               250      282.1721  0.7809162
# 0.3  3          0.8                50      292.6037  0.7653121
# 0.3  3          0.8               100      280.5611  0.7835391
# 0.3  3          0.8               150      277.9443  0.7874295
# 0.3  3          0.8               200      278.6056  0.7864819
# 0.3  3          0.8               250      277.5726  0.7882967
# 0.3  4          0.6                50      285.4327  0.7762741
# 0.3  4          0.6               100      280.9635  0.7830228
# 0.3  4          0.6               150      278.9734  0.7857260
# 0.3  4          0.6               200      279.1025  0.7854985
# 0.3  4          0.6               250      279.7801  0.7844868
# 0.3  4          0.8                50      278.6092  0.7863116
# 0.3  4          0.8               100      275.0605  0.7911958
# 0.3  4          0.8               150      276.0932  0.7895936
# 0.3  4          0.8               200      276.6932  0.7888152
# 0.3  4          0.8               250      276.7882  0.7886434
# 0.3  5          0.6                50      278.6389  0.7860189
# 0.3  5          0.6               100      277.3388  0.7878970
# 0.3  5          0.6               150      277.0158  0.7885310
# 0.3  5          0.6               200      277.9142  0.7872614
# 0.3  5          0.6               250      278.1637  0.7868437
# 0.3  5          0.8                50      274.4491  0.7926590
# 0.3  5          0.8               100      272.8361  0.7949843
# 0.3  5          0.8               150      273.0825  0.7944516
# 0.3  5          0.8               200      273.8799  0.7932439
# 0.3  5          0.8               250      273.8088  0.7932772
# 0.4  1          0.6                50      351.6987  0.6620388
# 0.4  1          0.6               100      339.3202  0.6831895
# 0.4  1          0.6               150      334.9410  0.6910319
# 0.4  1          0.6               200      333.4040  0.6936766
# 0.4  1          0.6               250      332.1922  0.6959332
# 0.4  1          0.8                50      350.4393  0.6648570
# 0.4  1          0.8               100      338.6715  0.6844041
# 0.4  1          0.8               150      334.1407  0.6926165
# 0.4  1          0.8               200      332.0675  0.6962225
# 0.4  1          0.8               250      331.2152  0.6976598
# 0.4  2          0.6                50      314.7701  0.7282529
# 0.4  2          0.6               100      299.2940  0.7542080
# 0.4  2          0.6               150      293.9698  0.7628434
# 0.4  2          0.6               200      292.7748  0.7646202
# 0.4  2          0.6               250      291.8499  0.7662358
# 0.4  2          0.8                50      312.0311  0.7328536
# 0.4  2          0.8               100      297.1224  0.7574031
# 0.4  2          0.8               150      293.1209  0.7636836
# 0.4  2          0.8               200      291.6264  0.7660292
# 0.4  2          0.8               250      290.5071  0.7678259
# 0.4  3          0.6                50      295.5548  0.7594648
# 0.4  3          0.6               100      289.7313  0.7686572
# 0.4  3          0.6               150      288.0264  0.7713004
# 0.4  3          0.6               200      289.4403  0.7695479
# 0.4  3          0.6               250      289.8178  0.7689598
# 0.4  3          0.8                50      291.7094  0.7661458
# 0.4  3          0.8               100      285.6522  0.7751662
# 0.4  3          0.8               150      284.6599  0.7766314
# 0.4  3          0.8               200      284.2684  0.7775205
# 0.4  3          0.8               250      285.7019  0.7753165
# 0.4  4          0.6                50      285.1814  0.7762253
# 0.4  4          0.6               100      283.2913  0.7789964
# 0.4  4          0.6               150      283.5319  0.7786678
# 0.4  4          0.6               200      283.9068  0.7779836
# 0.4  4          0.6               250      285.0428  0.7763475
# 0.4  4          0.8                50      285.4344  0.7765332
# 0.4  4          0.8               100      283.8257  0.7790016
# 0.4  4          0.8               150      286.3042  0.7750926
# 0.4  4          0.8               200      286.3280  0.7750099
# 0.4  4          0.8               250      287.3298  0.7734550
# 0.4  5          0.6                50      284.4841  0.7764801
# 0.4  5          0.6               100      286.3161  0.7740336
# 0.4  5          0.6               150      287.1262  0.7729405
# 0.4  5          0.6               200      287.2944  0.7727422
# 0.4  5          0.6               250      287.6909  0.7721295
# 0.4  5          0.8                50      285.6795  0.7749796
# 0.4  5          0.8               100      286.7548  0.7733996
# 0.4  5          0.8               150      288.0017  0.7713333
# 0.4  5          0.8               200      288.6103  0.7704986
# 0.4  5          0.8               250      288.9419  0.7700378
#
# Tuning parameter 'gamma' was held constant at a value of 0
# Tuning parameter 'min_child_weight' was held constant
# at a value of 1
# RMSE was used to select the optimal model using  the smallest value.
# The final values used for the model were nrounds = 100, max_depth = 5, eta = 0.3, gamma = 0, colsample_bytree =
#     0.8 and min_child_weight = 1.

preds <- predict(mod, x_test)
preds <- as.integer(round(preds))

submission <- data.frame(date = testset2$date, visitors_pool_total = preds)
submission <- submission %>% mutate(date = format(date, "%m/%d/%Y"))
submission$date <- gsub("^0", "", submission$date)
submission$date <- gsub("/0", "/", submission$date)

write_csv(submission, path = "submissions/xgb2.csv")

